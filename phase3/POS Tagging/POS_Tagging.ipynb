{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Part-of-Speech (POS) Tagging\n",
        "POS tagging is the process of labeling words with grammatical categories (noun, verb, etc.) based on context. It resolves ambiguity - e.g., \"book\" can be a noun (\"read a book\") or verb (\"book tickets\").\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Sentence: \"They fish in the river\"\n",
        "\n",
        "Tags: They/PRP | fish/VBP | in/IN | the/DT | river/NN\n",
        "\n",
        "PRP => Personal pronoun\n",
        "\n",
        "VBP => Verb, non-3rd person singular present\n",
        "\n",
        "IN => Preposition or subordinating conjunction\n",
        "\n",
        "DT => Determiner\n",
        "\n",
        "NN => Noun, singular or mass\n"
      ],
      "metadata": {
        "id": "EshnLGw0pco9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Hidden Markov Models (HMMs)\n",
        "### Key Components:\n",
        "- **Hidden States**: POS tags (NN, VB, etc.)\n",
        "- **Observations**: Words in sentences\n",
        "- **Transition Probabilities**: P(tag_i | tag_{i-1})\n",
        "- **Emission Probabilities**: P(word | tag)[2][5]\n",
        "\n",
        "### vs Markov Models:\n",
        "| Feature          | Markov Model          | HMM                   |\n",
        "|------------------|-----------------------|-----------------------|\n",
        "| State Visibility | Fully observable       | Hidden                |\n",
        "| Observations     | Same as states        | Different from states |\n",
        "| Complexity       | Simple transitions    | Transition + Emission |\n",
        "| Use Case         | Weather prediction    | Speech recognition    |\n",
        "\n",
        "**Example HMM for POS Tagging:**\n",
        "\n",
        "States: {NN, VB, DT} POS Tags (On Transitions)\n",
        "\n",
        "Observations: {\"the\", \"cat\", \"sits\"} Emissions\n",
        "\n",
        "Transition: P(VB|NN) = 0.3, P(NN|DT) = 0.6\n",
        "\n",
        "Emission: P(\"cat\"|NN) = 0.7, P(\"the\"|DT) = 0.9"
      ],
      "metadata": {
        "id": "pFiYJ9ZFqm8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Viterbi Algorithm\n",
        "### Why Needed:\n",
        "Computes the **most likely tag sequence** efficiently (O(nk²) vs O(kⁿ) brute force). Essential for decoding HMM states in realistic NLP tasks.\n",
        "\n",
        "Core Idea\n",
        "\n",
        "Given:\n",
        "\n",
        "1. Observations: Words in a sentence (e.g., [\"The\", \"cat\", \"sits\"])\n",
        "\n",
        "2.  Hidden States: POS tags (e.g., DT, NN, VB)\n",
        "\n",
        "3. Transition Probabilities: P(tagi∣tagi−1)\n",
        "\n",
        "4. Emission Probabilities: P(word∣tag)\n",
        "\n",
        "The algorithm computes the optimal tag sequence by combining these probabilities efficiently"
      ],
      "metadata": {
        "id": "Ajd_sPLmrRNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Steps\n",
        "1. **Initialization**\n",
        "\n",
        "Create two matrices:\n",
        "\n",
        "* Viterbi Table dp: Stores maximum probabilities\n",
        "* Backpointers bp: Tracks optimal previous states\n",
        "\n",
        "For the first word w1:\n",
        "dp[s]=P_initial(s)×P_emit(w1∣s)\n",
        "\n",
        "\n",
        "where s = all possible tags\n",
        "\n",
        "Word: \"The\"\n",
        "\n",
        "Tags: DT (0.8), NN (0.1)\n",
        "\n",
        "dp[0][DT] = 0.8 * 0.9 = 0.72  \n",
        "\n",
        "dp[0][NN] = 0.1 * 0.1 = 0.01\n",
        "\n",
        "2. **Recursion**\n",
        "\n",
        "For each subsequent word wt:\n",
        "\n",
        "dp[t][s]=max_⁡r∈tags(dp[t−1][r]×P_trans(s∣r)×P_emit(wt∣s))\n",
        "\n",
        "Update bp[t][s] with the best r\n",
        "\n",
        "Example:\n",
        "\n",
        "Word: \"cat\"\n",
        "\n",
        "Previous best: DT (0.72)\n",
        "\n",
        "dp[1][NN] = 0.72 * P(NN|DT) * P(\"cat\"|NN)\n",
        "\n",
        "           = 0.72 * 0.6 * 0.7 = 0.3024\n",
        "\n",
        "3. **Termination**\n",
        "\n",
        "Find the maximum probability in the last column:\n",
        "\n",
        "Best_Path_End=arg⁡max⁡_s(dp[n−1][s])\n",
        "\n",
        "4. **Backtracking**\n",
        "\n",
        "best_path = [best_final_tag]\n",
        "\n",
        "for t in reversed(range(1, n)):\n",
        "\n",
        "    best_path.insert(0, bp[t][best_path[0]])\n",
        "\n"
      ],
      "metadata": {
        "id": "_9dLmNOMsHMU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Orw7RaLpXu7",
        "outputId": "3ffdf11d-88c7-400d-d1ca-7fdaa342b8a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required resources\n",
        "import nltk\n",
        "nltk.download('treebank')\n",
        "nltk.download('punkt')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXblZBP4xP9A",
        "outputId": "a4a70ca9-fa03-4792-f756-c665e196ca1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Required Libraries\n",
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict # It will helpful in counting\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "CP9-nR_oxTqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare dataset\n",
        "tagged_sentences = treebank.tagged_sents(tagset='universal')  # Using universal tags\n",
        "train_sentences, test_sentences = train_test_split(tagged_sentences, test_size=0.1, random_state=30)\n",
        "train_sentences, val_sentences = train_test_split(train_sentences, test_size=0.1, random_state=30)\n",
        "\n",
        "print(f\"Total length of the dataset: {len(tagged_sentences)}\")\n",
        "print(\"Random Sample: \")\n",
        "print(train_sentences[12])\n",
        "print(f\"Training Sample: {len(train_sentences)}\")\n",
        "print(f\"Validation Sample: {len(val_sentences)}\")\n",
        "print(f\"Test sample: {len(test_sentences)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLmwTyLHyv9W",
        "outputId": "d8f192fa-615c-483b-d4d2-169f1ae40402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total length of the dataset: 3914\n",
            "Random Sample: \n",
            "[('We', 'PRON'), ('also', 'ADV'), ('know', 'VERB'), ('that', 'ADP'), ('the', 'DET'), ('tort', 'NOUN'), ('system', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('lousy', 'ADJ'), ('way', 'NOUN'), ('0', 'X'), ('*', 'X'), ('to', 'PRT'), ('compensate', 'VERB'), ('victims', 'NOUN'), ('*T*-1', 'X'), ('anyway', 'ADV'), (';', '.'), ('some', 'DET'), ('win', 'VERB'), ('the', 'DET'), ('legal', 'ADJ'), ('lottery', 'NOUN'), (',', '.'), ('others', 'NOUN'), ('get', 'VERB'), ('much', 'ADV'), ('less', 'ADJ'), ('and', 'CONJ'), ('contingency-fee', 'ADJ'), ('lawyers', 'NOUN'), ('take', 'VERB'), ('a', 'DET'), ('big', 'ADJ'), ('cut', 'NOUN'), ('either', 'DET'), ('way', 'NOUN'), ('.', '.')]\n",
            "Training Sample: 3169\n",
            "Validation Sample: 353\n",
            "Test sample: 392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_probabilities(sentences, alpha=1, beta=1):\n",
        "    # === Data Collection Phase ===\n",
        "    # Identify all unique tags and words in the training data\n",
        "    all_tags = set()\n",
        "    all_words = set()\n",
        "    for sentence in sentences:\n",
        "        for word, tag in sentence:\n",
        "            all_tags.add(tag)    # Collect POS tags\n",
        "            all_words.add(word)  # Collect vocabulary\n",
        "    all_tags = list(all_tags)    # Convert to list for ordered access\n",
        "\n",
        "    # === Counting Initialization ===\n",
        "    # Create data structures for storing counts\n",
        "    transition_counts = defaultdict(lambda: defaultdict(int))  # tag_i-1 → tag_i counts\n",
        "    emission_counts = defaultdict(lambda: defaultdict(int))    # tag → word counts\n",
        "    tag_counts = defaultdict(int)                              # tag → total occurrences\n",
        "\n",
        "    # === Counting with Smoothing ===\n",
        "    for sentence in sentences:\n",
        "        prev_tag = '<s>'  # Start with sentence beginning marker\n",
        "\n",
        "        for word, tag in sentence:\n",
        "            # Transition smoothing: Add alpha to all possible transitions\n",
        "            for t in all_tags:\n",
        "                transition_counts[prev_tag][t] += alpha\n",
        "            # Actual observed transition count\n",
        "            transition_counts[prev_tag][tag] += 1  # Overwrite smoothed value\n",
        "\n",
        "            # Emission smoothing: Add beta to all possible words (implicit)\n",
        "            emission_counts[tag][word] += beta\n",
        "            tag_counts[tag] += 1  # Total count for this tag\n",
        "\n",
        "            prev_tag = tag  # Move to next position\n",
        "\n",
        "        # End-of-sentence transition\n",
        "        transition_counts[prev_tag]['</s>'] += 1\n",
        "\n",
        "    # === Transition Probability Calculation ===\n",
        "    transition_prob = defaultdict(dict)\n",
        "    for prev_tag in transition_counts:\n",
        "        # Total transitions from prev_tag (with smoothing denominator)\n",
        "        total = sum(transition_counts[prev_tag].values()) + alpha * len(all_tags)\n",
        "\n",
        "        for curr_tag in transition_counts[prev_tag]:\n",
        "            # Subtract alpha because we added it during smoothing initialization\n",
        "            prob = (transition_counts[prev_tag][curr_tag] - alpha) / total\n",
        "            transition_prob[prev_tag][curr_tag] = prob\n",
        "\n",
        "    # === Emission Probability Calculation ===\n",
        "    emission_prob = defaultdict(dict)\n",
        "    for tag in emission_counts:\n",
        "        # Total emissions for tag (with smoothing denominator)\n",
        "        total = tag_counts[tag] + beta * len(all_words)\n",
        "\n",
        "        for word in emission_counts[tag]:\n",
        "            emission_prob[tag][word] = emission_counts[tag][word] / total\n",
        "\n",
        "    return transition_prob, emission_prob, all_tags\n"
      ],
      "metadata": {
        "id": "HfBEpMIDyoig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def viterbi(sentence, transition_prob, emission_prob, all_tags):\n",
        "    \"\"\"\n",
        "    Finds the most probable POS tag sequence using dynamic programming.\n",
        "\n",
        "    Args:\n",
        "        sentence: List of words to tag\n",
        "        transition_prob: Precomputed transition probabilities\n",
        "        emission_prob: Precomputed emission probabilities\n",
        "        all_tags: List of all possible POS tags\n",
        "\n",
        "    Returns:\n",
        "        List of predicted tags for the input sentence\n",
        "    \"\"\"\n",
        "\n",
        "    # === Initialization ===\n",
        "    T = len(sentence)  # Number of words in the sentence\n",
        "    num_tags = len(all_tags)\n",
        "\n",
        "    # DP table: V[t][i] = max probability of being in tag i at position t\n",
        "    V = np.zeros((T, num_tags))\n",
        "\n",
        "    # Backpointer table: tracks index of best previous tag\n",
        "    backpointer = np.zeros((T, num_tags), dtype=int)\n",
        "\n",
        "    # Initialize first column (t=0)\n",
        "    for i, tag in enumerate(all_tags):\n",
        "        # Probability = P(start -> tag) * P(word0 | tag)\n",
        "        V[0][i] = transition_prob['<s>'].get(tag, 1e-6) * \\\n",
        "                 emission_prob[tag].get(sentence[0], 1e-6)\n",
        "\n",
        "    # === Forward Pass ===\n",
        "    # Fill DP table from left to right\n",
        "    for t in range(1, T):  # For each word position\n",
        "        for i, curr_tag in enumerate(all_tags):  # Current tag candidates\n",
        "\n",
        "            max_prob = -1\n",
        "            best_prev_idx = 0\n",
        "\n",
        "            # Find best previous tag\n",
        "            for j, prev_tag in enumerate(all_tags):\n",
        "                # Transition prob: prev_tag -> curr_tag\n",
        "                trans_p = transition_prob[prev_tag].get(curr_tag, 1e-6)\n",
        "\n",
        "                # Emission prob: current word given curr_tag\n",
        "                emit_p = emission_prob[curr_tag].get(sentence[t], 1e-6)\n",
        "\n",
        "                # Current probability calculation\n",
        "                current_prob = V[t-1][j] * trans_p * emit_p\n",
        "\n",
        "                # Track maximum probability\n",
        "                if current_prob > max_prob:\n",
        "                    max_prob = current_prob\n",
        "                    best_prev_idx = j\n",
        "\n",
        "            # Store results\n",
        "            V[t][i] = max_prob\n",
        "            backpointer[t][i] = best_prev_idx\n",
        "\n",
        "    # === Backtracking ===\n",
        "    # Reconstruct optimal path from DP tables\n",
        "    best_path = []\n",
        "\n",
        "    # Start from last word's best tag\n",
        "    best_last_idx = np.argmax(V[-1])\n",
        "    best_path.append(all_tags[best_last_idx])\n",
        "\n",
        "    # Trace back through backpointers\n",
        "    for t in range(T-1, 0, -1):\n",
        "        best_prev_idx = backpointer[t][best_last_idx]\n",
        "        best_path.insert(0, all_tags[best_prev_idx])  # Add to front\n",
        "        best_last_idx = best_prev_idx\n",
        "\n",
        "    return best_path\n"
      ],
      "metadata": {
        "id": "iWYv6rm_y_sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation function\n",
        "def evaluate(model, test_data):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for sentence in test_data:\n",
        "        words = [word for word, tag in sentence]\n",
        "        true_tags = [tag for word, tag in sentence]\n",
        "        pred_tags = viterbi(words, *model)\n",
        "        correct += sum(1 for t, p in zip(true_tags, pred_tags) if t == p)\n",
        "        total += len(sentence)\n",
        "    return correct / total\n",
        "\n"
      ],
      "metadata": {
        "id": "pN7HQJ5gzFSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "print(\"Validation Accuracy:\", evaluate((transition_prob, emission_prob, all_tags), val_sentences))\n",
        "print(\"Test Accuracy:\", evaluate((transition_prob, emission_prob, all_tags), test_sentences))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXwZUL4QzHz_",
        "outputId": "12bd42ca-8778-401c-e64c-7a25e7282e29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9238095238095239\n",
            "Test Accuracy: 0.929010170174202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference example\n",
        "def tag_sentence(sentence):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tags = viterbi(tokens, transition_prob, emission_prob, all_tags)\n",
        "    return list(zip(tokens, tags))\n",
        "\n",
        "sample_sentence = \"Ram is a good boy\"\n",
        "print(\"\\nExample Tagging:\")\n",
        "print(tag_sentence(sample_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7GRc6cBzK4J",
        "outputId": "93016648-613c-49c0-de10-78ed9b72bf73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example Tagging:\n",
            "[('Ram', 'PRON'), ('is', 'VERB'), ('a', 'DET'), ('good', 'ADJ'), ('boy', 'NOUN')]\n"
          ]
        }
      ]
    }
  ]
}